{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Lambda School Data Science — Tree Ensembles_ \n",
    "\n",
    "# Random Forests\n",
    "\n",
    "### Pre-read / pre-watch\n",
    "- [Scikit-Learn User Guide, Ensemble Methods](https://scikit-learn.org/stable/modules/ensemble.html)\n",
    "- [Coloring with Random Forests](http://structuringtheunstructured.blogspot.com/2017/11/coloring-with-random-forests.html)\n",
    "- [Beware Default Random Forest Importances](https://explained.ai/rf-importance/index.html)\n",
    "\n",
    "### More\n",
    "- [Machine Learning Explainability: Permutation Importance](https://www.kaggle.com/dansbecker/permutation-importance)\n",
    "- [eli5: Permutation Importance](https://eli5.readthedocs.io/en/latest/blackbox/permutation_importance.html)\n",
    "- [eli5: Explaining XGBoost predictions on the Titanic dataset](https://eli5.readthedocs.io/en/latest/_notebooks/xgboost-titanic.html)\n",
    "- [The Mechanics of Machine Learning: Categorically Speaking](https://mlbook.explained.ai/catvars.html)\n",
    "\n",
    "\n",
    "[Selecting good features – Part III: random forests](https://blog.datadive.net/selecting-good-features-part-iii-random-forests/)\n",
    ">There are a few things to keep in mind when using the impurity based ranking. Firstly, feature selection based on impurity reduction is biased towards preferring variables with more categories. \n",
    ">\n",
    ">Secondly, when the dataset has two (or more) correlated features, then from the point of view of the model, any of these correlated features can be used as the predictor, with no concrete preference of one over the others. But once one of them is used, the importance of others is significantly reduced since effectively the impurity they can remove is already removed by the first feature. As a consequence, they will have a lower reported importance. This is not an issue when we want to use feature selection to reduce overfitting, since it makes sense to remove features that are mostly duplicated by other features. But when interpreting the data, it can lead to the incorrect conclusion that one of the variables is a strong predictor while the others in the same group are unimportant, while actually they are very close in terms of their relationship with the response variable.\n",
    "\n",
    "\n",
    "[An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/), Chapter 8.2.1, Out-of-Bag Error Estimation\n",
    "\n",
    "> It turns out that **there is a very straightforward way to estimate the test error of a bagged model, without the need to perform cross-validation or the validation set approach.** \n",
    ">\n",
    "> Recall that the key to bagging is that trees are repeatedly fit to bootstrapped subsets of the observations. One can show that on average, each bagged tree makes use of around two-thirds of the observations. The remaining one-third of the **observations not used to fit a given bagged tree are referred to as the out-of bag (OOB) observations.**\n",
    ">\n",
    ">We can predict the response for the ith observation using each of the trees in which that observation was OOB. This will yield around B/3 predictions for the ith observation. In order to obtain a single prediction for the ith observation, we can average these predicted responses (if regression is the goal) or can take a majority vote (if classification is the goal). \n",
    ">\n",
    ">This leads to a single OOB prediction for the ith observation. An OOB prediction can be obtained in this way for each of the n observations, from which the overall OOB MSE (for a regression problem) or classification error (for a classification problem) can be computed. The resulting **OOB error is a valid estimate of the test error for the bagged model, since the response for each observation is predicted using only the trees that were not fit using that observation.** ... \n",
    ">\n",
    ">It can be shown that with B sufficiently large, OOB error is virtually equivalent to leave-one-out cross-validation error. The OOB approach for estimating the test error is particularly **convenient when performing bagging on large data sets for which cross-validation would be computationally onerous.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "- [eli5](https://github.com/TeamHG-Memex/eli5): `conda install -c conda-forge eli5` / `pip install eli5`\n",
    "- [category_encoders](https://github.com/scikit-learn-contrib/categorical-encoding): `conda install -c conda-forge category_encoders` / `pip install category_encoders`\n",
    "- [mlxtend](https://github.com/rasbt/mlxtend): `pip install mlxtend`\n",
    "- [ipywidgets](https://ipywidgets.readthedocs.io/en/stable/examples/Using%20Interact.html): included with Anaconda, doesn't work on Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ipywidgets revisited: Decision Tree vs Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressing a wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Example from http://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html\n",
    "def make_data():\n",
    "    import numpy as np\n",
    "    rng = np.random.RandomState(1)\n",
    "    X = np.sort(5 * rng.rand(80, 1), axis=0)\n",
    "    y = np.sin(X).ravel()\n",
    "    y[::5] += 2 * (0.5 - rng.rand(16))\n",
    "    return X, y\n",
    "\n",
    "X, y = make_data()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "def regress_wave(max_depth):\n",
    "    dt = DecisionTreeRegressor(max_depth=max_depth)\n",
    "    dt.fit(X_train, y_train)\n",
    "    print('Decision Tree train R^2:', dt.score(X_train, y_train))\n",
    "    print('Decision Tree test R^2:', dt.score(X_test, y_test))\n",
    "    plt.scatter(X_train, y_train)\n",
    "    plt.scatter(X_test, y_test)\n",
    "    plt.step(X, dt.predict(X))\n",
    "    plt.show()\n",
    "    \n",
    "    rf = RandomForestRegressor(max_depth=max_depth, n_estimators=100, n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    print('Random Forest train R^2:', rf.score(X_train, y_train))\n",
    "    print('Random Forest test R^2:', rf.score(X_test, y_test))\n",
    "    plt.scatter(X_train, y_train)\n",
    "    plt.scatter(X_test, y_test)\n",
    "    plt.step(X, rf.predict(X))\n",
    "    plt.show()\n",
    "    \n",
    "interact(regress_wave, max_depth=(1,8,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titanic survival, by Age & Fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "titanic = sns.load_dataset('titanic')\n",
    "X = SimpleImputer().fit_transform(titanic[['age', 'fare']])\n",
    "y = titanic['survived'].values\n",
    "\n",
    "def classify_titanic(max_depth):\n",
    "    dt = DecisionTreeClassifier(max_depth=max_depth)\n",
    "    dt.fit(X, y)\n",
    "    plot_decision_regions(X, y, dt)\n",
    "    plt.title('Decision Tree')\n",
    "    plt.axis((0,75,0,175))\n",
    "    plt.show()\n",
    "\n",
    "    rf = RandomForestClassifier(max_depth=max_depth, n_estimators=100, n_jobs=-1)\n",
    "    rf.fit(X, y)\n",
    "    plot_decision_regions(X, y, rf)\n",
    "    plt.title('Random Forest')\n",
    "    plt.axis((0,75,0,175))\n",
    "    plt.show()\n",
    "    \n",
    "interact(classify_titanic, max_depth=(1,8,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lending Club"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read csv files downloaded from [Kaggle](https://www.kaggle.com/c/ds1-tree-ensembles/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_columns = 200\n",
    "pd.options.display.max_rows = 200\n",
    "\n",
    "X_train = pd.read_csv('train_features.csv')\n",
    "X_test = pd.read_csv('test_features.csv')\n",
    "y_train = pd.read_csv('train_labels.csv')['charged_off']\n",
    "y_test = pd.read_csv('test_labels.csv')['charged_off']\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrangle X_train and X_test in the same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle(X):\n",
    "    X = X.copy()\n",
    "    \n",
    "    # Drop some columns\n",
    "    X = X.drop(columns='id')  # id is random\n",
    "    X = X.drop(columns=['member_id', 'url', 'desc'])  # All null\n",
    "    X = X.drop(columns='title')  # Duplicative of purpose\n",
    "    X = X.drop(columns='grade')  # Duplicative of sub_grade\n",
    "    \n",
    "    # Transform sub_grade from \"A1\" - \"G5\" to 1.1 - 7.5\n",
    "    def wrangle_sub_grade(x):\n",
    "        first_digit = ord(x[0]) - 64\n",
    "        second_digit = int(x[1])\n",
    "        return first_digit + second_digit/10\n",
    "    \n",
    "    X['sub_grade'] = X['sub_grade'].apply(wrangle_sub_grade)\n",
    "\n",
    "    # Convert percentages from strings to floats\n",
    "    X['int_rate'] = X['int_rate'].str.strip('%').astype(float)\n",
    "    X['revol_util'] = X['revol_util'].str.strip('%').astype(float)\n",
    "        \n",
    "    # Transform earliest_cr_line to an integer: how many days it's been open\n",
    "    X['earliest_cr_line'] = pd.to_datetime(X['earliest_cr_line'], infer_datetime_format=True)\n",
    "    X['earliest_cr_line'] = pd.Timestamp.today() - X['earliest_cr_line']\n",
    "    X['earliest_cr_line'] = X['earliest_cr_line'].dt.days\n",
    "    \n",
    "    # Create features for three employee titles: teacher, manager, owner\n",
    "    X['emp_title'] = X['emp_title'].str.lower()\n",
    "    X['emp_title_teacher'] = X['emp_title'].str.contains('teacher', na=False)\n",
    "    X['emp_title_manager'] = X['emp_title'].str.contains('manager', na=False)\n",
    "    X['emp_title_owner']   = X['emp_title'].str.contains('owner', na=False)\n",
    "    \n",
    "    # Drop categoricals with high cardinality\n",
    "    X = X.drop(columns=['emp_title', 'zip_code'])\n",
    "    \n",
    "    # Transform features with many nulls to binary flags\n",
    "    many_nulls = ['sec_app_mths_since_last_major_derog',\n",
    "                  'sec_app_revol_util',\n",
    "                  'sec_app_earliest_cr_line',\n",
    "                  'sec_app_mort_acc',\n",
    "                  'dti_joint',\n",
    "                  'sec_app_collections_12_mths_ex_med',\n",
    "                  'sec_app_chargeoff_within_12_mths',\n",
    "                  'sec_app_num_rev_accts',\n",
    "                  'sec_app_open_act_il',\n",
    "                  'sec_app_open_acc',\n",
    "                  'revol_bal_joint',\n",
    "                  'annual_inc_joint',\n",
    "                  'sec_app_inq_last_6mths',\n",
    "                  'mths_since_last_record',\n",
    "                  'mths_since_recent_bc_dlq',\n",
    "                  'mths_since_last_major_derog',\n",
    "                  'mths_since_recent_revol_delinq',\n",
    "                  'mths_since_last_delinq',\n",
    "                  'il_util',\n",
    "                  'emp_length',\n",
    "                  'mths_since_recent_inq',\n",
    "                  'mo_sin_old_il_acct',\n",
    "                  'mths_since_rcnt_il',\n",
    "                  'num_tl_120dpd_2m',\n",
    "                  'bc_util',\n",
    "                  'percent_bc_gt_75',\n",
    "                  'bc_open_to_buy',\n",
    "                  'mths_since_recent_bc']\n",
    "\n",
    "    for col in many_nulls:\n",
    "        X[col] = X[col].isnull()\n",
    "    \n",
    "    # For features with few nulls, do mean imputation\n",
    "    for col in X:\n",
    "        if X[col].isnull().sum() > 0:\n",
    "            X[col] = X[col].fillna(X[col].mean())\n",
    "    \n",
    "    # Return the wrangled dataframe\n",
    "    return X\n",
    "\n",
    "\n",
    "X_train = wrangle(X_train)\n",
    "X_test  = wrangle(X_test)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now X_train (and X_test) have no nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_counts = X_train.isnull().sum()\n",
    "all(null_counts == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And no high cardinality categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cardinality = X_train.select_dtypes(exclude='number').nunique()\n",
    "all(cardinality <= 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "pipe = make_pipeline(\n",
    "    ce.OrdinalEncoder(), \n",
    "    DecisionTreeClassifier(max_depth=5, class_weight='balanced')\n",
    ")\n",
    "\n",
    "cross_val_score(pipe, X_train, y_train, cv=5, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "Improves ROC AUC compared to Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pipe = make_pipeline(\n",
    "    ce.OrdinalEncoder(), \n",
    "    RandomForestClassifier(\n",
    "        n_estimators=100, \n",
    "        class_weight='balanced', \n",
    "        min_samples_leaf=0.005, \n",
    "        oob_score=True, \n",
    "        n_jobs=-1)\n",
    ")\n",
    "\n",
    "cross_val_score(pipe, X_train, y_train, cv=5, scoring='roc_auc', verbose=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-Bag estimated score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out-of-bag is a faster way to get an estimated score with Random Forest, using the parameter `oob_score=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred_proba = pipe.named_steps['randomforestclassifier'].oob_decision_function_[:, 1]\n",
    "print('ROC AUC, Out-of-Bag estimate:', roc_auc_score(y_train, y_pred_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can explore hyperparameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "max_depths = list(range(2, 12, 2)) + [None]\n",
    "\n",
    "for max_depth in max_depths:\n",
    "    \n",
    "    pipe = make_pipeline(\n",
    "        ce.OrdinalEncoder(), \n",
    "        RandomForestClassifier(\n",
    "            n_estimators=100, \n",
    "            class_weight='balanced', \n",
    "            max_depth=max_depth, \n",
    "            oob_score=True, \n",
    "            n_jobs=-1\n",
    "        )\n",
    "    )\n",
    "        \n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred_proba = pipe.named_steps['randomforestclassifier'].oob_decision_function_[:, 1]\n",
    "    print('Max Depth:', max_depth)\n",
    "    print('ROC AUC, OOB:', roc_auc_score(y_train, y_pred_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at feature importances. [But remember:](https://blog.datadive.net/selecting-good-features-part-iii-random-forests/)\n",
    "\n",
    ">Firstly, feature selection based on impurity reduction is biased towards preferring variables with more categories.\n",
    ">\n",
    ">Secondly, when the dataset has two (or more) correlated features, then from the point of view of the model, any of these correlated features can be used as the predictor, with no concrete preference of one over the others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_feature_importances(\n",
    "    pipe, X, y, estimator_name='randomforestclassifier', \n",
    "    n=20, figsize=(8, 8)):\n",
    "    \n",
    "    # pipe must not change dimensions of X dataframe\n",
    "    pipe.fit(X, y)\n",
    "    \n",
    "    importances = pd.Series(\n",
    "        pipe.named_steps[estimator_name].feature_importances_, \n",
    "        X.columns)\n",
    "\n",
    "    top_n = importances.sort_values(ascending=False)[:n]\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    top_n.sort_values().plot.barh(color='grey')\n",
    "\n",
    "    \n",
    "show_feature_importances(pipe, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Column Importance / \"Ablation Study\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sub_grade` and `int_rate` are highly correlated. If we drop one of those features, the model uses the other more, so the score remains similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(pipe, X_train.drop(columns='sub_grade'), y_train, cv=5, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_feature_importances(pipe, X_train.drop(columns='sub_grade'), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we drop _both_ features, then the score decreases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(pipe, X_train.drop(columns=['sub_grade', 'int_rate']), y_train, cv=5, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information, see [Beware Default Random Forest Importances](https://explained.ai/rf-importance/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutation Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Permutation Importance is a compromise between Feature Importance based on impurity reduction (which is the fastest) and Drop Column Importance (which is the \"best.\")\n",
    "\n",
    "[The ELI5 library documentation explains,](https://eli5.readthedocs.io/en/latest/blackbox/permutation_importance.html)\n",
    "\n",
    "> Importance can be measured by looking at how much the score (accuracy, F1, R^2, etc. - any score we’re interested in) decreases when a feature is not available.\n",
    ">\n",
    "> To do that one can remove feature from the dataset, re-train the estimator and check the score. But it requires re-training an estimator for each feature, which can be computationally intensive. ...\n",
    ">\n",
    ">To avoid re-training the estimator we can remove a feature only from the test part of the dataset, and compute score without using this feature. It doesn’t work as-is, because estimators expect feature to be present. So instead of removing a feature we can replace it with random noise - feature column is still there, but it no longer contains useful information. This method works if noise is drawn from the same distribution as original feature values (as otherwise estimator may fail). The simplest way to get such noise is to shuffle values for a feature, i.e. use other examples’ feature values - this is how permutation importance is computed.\n",
    ">\n",
    ">The method is most suitable for computing feature importances when a number of columns (features) is not huge; it can be resource-intensive otherwise.\n",
    "\n",
    "For more documentation on using this library, see:\n",
    "- [eli5.sklearn.PermutationImportance](https://eli5.readthedocs.io/en/latest/autodocs/sklearn.html#eli5.sklearn.permutation_importance.PermutationImportance)\n",
    "- [eli5.show_weights](https://eli5.readthedocs.io/en/latest/autodocs/eli5.html#eli5.show_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "encoder = ce.OrdinalEncoder()\n",
    "X_train_transformed = encoder.fit_transform(X_train)\n",
    "\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100, \n",
    "    class_weight='balanced', \n",
    "    min_samples_leaf=0.005, \n",
    "    n_jobs=-1)\n",
    "\n",
    "model.fit(X_train_transformed, y_train)\n",
    "permuter = PermutationImportance(model, scoring='roc_auc', n_iter=1, cv='prefit')\n",
    "permuter.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_weights(permuter, top=None, feature_names=X_train_transformed.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Permutation Importance weights for feature selection. For example, we can remove features with zero weight. The model trains faster and the score does not decrease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = X_train.columns[permuter.feature_importances_ > 0]\n",
    "cross_val_score(pipe, X_train[subset], y_train, cv=5, scoring='roc_auc', verbose=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
